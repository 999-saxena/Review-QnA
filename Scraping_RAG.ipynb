{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade streamlit pyngrok -q\n",
        "!pip install pinecone -q\n",
        "!pip install --upgrade pydantic -q\n",
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "6c1suuLInZbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98ac65b-ca86-4e2b-c11d-6a1d353a47b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2wOlwfSCyrR717tnSJ9QLwrzFPx_6FEiQuNMhmGtQRFnkyie1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfTXYS3lN5M",
        "outputId": "922ac129-a882-46cc-d664-384038b182d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import pinecone\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModel\n",
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "st.title('Review QnA')\n",
        "\n",
        "def scrape_reviews(company_name, sources=['bestcompany', 'trustpilot'], num_pages=5, delay=2):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    all_reviews = []\n",
        "\n",
        "    source_configs = {\n",
        "        'bestcompany': {\n",
        "            'url_template': f'https://bestcompany.com/health-insurance/{company_name}?page=',\n",
        "            'selector': ('div', {'class_': 'whitespace-pre-line break-words'}),\n",
        "        },\n",
        "        'trustpilot': {\n",
        "            'url_template': f'https://www.trustpilot.com/review/www.{company_name}.com?page=',\n",
        "            'selector': ('p', {'attrs': {'data-service-review-text-typography': 'true'}}),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for source in sources:\n",
        "        if source not in source_configs:\n",
        "            print(f\"Source '{source}' is not supported.\")\n",
        "            continue\n",
        "\n",
        "        config = source_configs[source]\n",
        "        url_template = config['url_template']\n",
        "        selector_type, selector_attrs = config['selector']\n",
        "\n",
        "        def scrape_page(url):\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                if selector_type == 'p' and 'attrs' in selector_attrs:\n",
        "                    reviews = soup.find_all(selector_type, **selector_attrs['attrs'])\n",
        "                else:\n",
        "                    reviews = soup.find_all(selector_type, **selector_attrs)\n",
        "                return [review.get_text() for review in reviews]\n",
        "            else:\n",
        "                print(f\"Error scraping {url}. Status code: {response.status_code}\")\n",
        "                return []\n",
        "\n",
        "        url = url_template + str(1)\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            st.write(f\"Company '{company_name}' not found on {source}.\")\n",
        "            continue\n",
        "\n",
        "        source_reviews = []\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = url_template + str(page)\n",
        "            st.write(f\"Scraping {source} page {page}...\")\n",
        "            reviews = scrape_page(url)\n",
        "            if not reviews:\n",
        "                break\n",
        "            source_reviews.extend(reviews)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        all_reviews.extend(source_reviews)\n",
        "        print(f\"Added {len(source_reviews)} reviews from {source}\")\n",
        "\n",
        "    return all_reviews\n",
        "\n",
        "@st.cache_resource\n",
        "def models():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    embedding_tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\n",
        "    embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True).to(device)\n",
        "\n",
        "    model_path = \"microsoft/Phi-4-mini-instruct\"\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"cuda\",\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    st.write(\"Models loaded\")\n",
        "    return embedding_model, embedding_tokenizer, chat_model, chat_tokenizer\n",
        "\n",
        "def vector_db():\n",
        "    pine_key = \"pcsk_6CjF3M_TVzGxF6Fnzq9mD1eFk33hRRRSrjysqAP4ndVRDNGxNfFkeKw3tq9TtwLz2BhuQT\"\n",
        "    pc = Pinecone(api_key=pine_key)\n",
        "    index_name = 'store-embeddings'\n",
        "\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1024,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "    time.sleep(1)\n",
        "    return index\n",
        "\n",
        "def generate_embeddings(texts, embedding_model, embedding_tokenizer):\n",
        "    embedding_model.eval()\n",
        "    with torch.no_grad():\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        inputs = embedding_tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "        outputs = embedding_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().float().numpy()\n",
        "    return embeddings\n",
        "\n",
        "def upserting(all_reviews, index):\n",
        "    batch_size = 32\n",
        "    text_splitter = SpacyTextSplitter(chunk_size=500)\n",
        "\n",
        "    for review_id, review in enumerate(all_reviews):\n",
        "        chunks = text_splitter.split_text(review)\n",
        "        chunk_embeddings = []\n",
        "        chunk_ids = []\n",
        "        chunk_metadata = []\n",
        "\n",
        "        for chunk_index, chunk in enumerate(chunks):\n",
        "            chunk_embeddings.append(chunk)\n",
        "            chunk_ids.append(f\"{review_id}_{chunk_index}\")\n",
        "            chunk_metadata.append({\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk})\n",
        "\n",
        "            if len(chunk_embeddings) == batch_size:\n",
        "                embeddings = generate_embeddings(chunk_embeddings)\n",
        "                vectors = list(zip(chunk_ids, embeddings, chunk_metadata))\n",
        "                index.upsert(vectors)\n",
        "                chunk_embeddings, chunk_ids, chunk_metadata = [], [], []\n",
        "\n",
        "        if chunk_embeddings:\n",
        "            embeddings = generate_embeddings(chunk_embeddings)\n",
        "            vectors = list(zip(chunk_ids, embeddings, chunk_metadata))\n",
        "            index.upsert(vectors)\n",
        "\n",
        "    st.write(\"Upserting Completed\")\n",
        "\n",
        "def retrieve_context(query, index, embedding_model):\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = index.query(vector = query_embedding, top_k=5, include_metadata=True)\n",
        "    retrieved_text = \" \".join([result['metadata']['text'] for result in results['matches']])\n",
        "    return retrieved_text\n",
        "\n",
        "def generate_response(query, retrieved_context, chat_model, chat_tokenizer):\n",
        "    prompt = f\"\"\"### Context:\n",
        "            {retrieved_context}\n",
        "\n",
        "    ### Instructions:\n",
        "    You are a helpful assistant answering questions based solely on the provided context.\n",
        "    Respond in the third person without referencing individuals in the context.\n",
        "    If the information needed to answer the question is not in the context, respond with \"I cannot comment on that\".\n",
        "    Do not use any knowledge beyond what is in the context.\n",
        "\n",
        "    ### Question:\n",
        "    {query}\n",
        "\n",
        "    ### Answer:\n",
        "    \"\"\"\n",
        "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = chat_model.generate(**inputs, max_new_tokens=150, temperature = 0.1, do_sample=True)\n",
        "    return chat_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# UI input\n",
        "company = st.text_input(\"Enter Company Name\")\n",
        "\n",
        "if company:\n",
        "    reviews = scrape_reviews(company, sources=['bestcompany', 'trustpilot'], num_pages=1)\n",
        "    st.write(f\"Total reviews scraped: {len(reviews)}\")\n",
        "\n",
        "    if reviews:\n",
        "        embedding_model, embedding_tokenizer, chat_model, chat_tokenizer = models()\n",
        "        print(\"Models Loaded\")\n",
        "        index = vector_db()\n",
        "        # upserting(reviews, index)\n",
        "\n",
        "        query = st.text_input(\"Enter a query\")\n",
        "        if query:\n",
        "             context = retrieve_context(query, index, embedding_model)\n",
        "            #st.write(context)\n",
        "            #st.write(\"Retrieval temporarily disabled.\")\n",
        "             response = generate_response(query, context, chat_model, chat_tokenizer)\n",
        "             st.write(\"Respomse\")\n",
        "             st.write(response)\n",
        "        else:\n",
        "            st.write(\"Please enter a query\")\n",
        "else:\n",
        "    st.write(\"Please enter company name\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSwVGFhrX0s9",
        "outputId": "40b2acd0-93d6-4e50-8928-a494d0f4ced1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "v0adNzi3-hKP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "!streamlit run app.py &> streamlit_log.txt &\n",
        "\n",
        "public_url = ngrok.connect(8501, bind_tls = True)\n",
        "print(\"Streamlit app is live at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF3XUHvMYFOR",
        "outputId": "163e17e0-0174-45c6-f8fd-d4893f71aa4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://7245-34-73-73-169.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "import torch\n",
        "\n",
        "\n",
        "# Function to scrape reviews\n",
        "def scrape_reviews(company_name, sources=['bestcompany', 'trustpilot'], num_pages=5, delay=2):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    all_reviews = []\n",
        "\n",
        "    source_configs = {\n",
        "        'bestcompany': {\n",
        "            'url_template': f'https://bestcompany.com/health-insurance/{company_name}?page=',\n",
        "            'selector': ('div', {'class_': 'whitespace-pre-line break-words'}),\n",
        "        },\n",
        "        'trustpilot': {\n",
        "            'url_template': f'https://www.trustpilot.com/review/www.{company_name}.com?page=',\n",
        "            'selector': ('p', {'attrs': {'data-service-review-text-typography': 'true'}}),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for source in sources:\n",
        "        if source not in source_configs:\n",
        "            print(f\"Source '{source}' is not supported.\")\n",
        "            continue\n",
        "\n",
        "        config = source_configs[source]\n",
        "        url_template = config['url_template']\n",
        "        selector_type, selector_attrs = config['selector']\n",
        "\n",
        "        def scrape_page(url):\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                if selector_type == 'p' and 'attrs' in selector_attrs:\n",
        "                    reviews = soup.find_all(selector_type, **selector_attrs['attrs'])\n",
        "                else:\n",
        "                    reviews = soup.find_all(selector_type, **selector_attrs)\n",
        "                return [review.get_text() for review in reviews]\n",
        "            else:\n",
        "                print(f\"Error scraping {url}. Status code: {response.status_code}\")\n",
        "                return []\n",
        "\n",
        "        url = url_template + str(1)\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            st.write(f\"Company '{company_name}' not found on {source}.\")\n",
        "            continue\n",
        "\n",
        "        source_reviews = []\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = url_template + str(page)\n",
        "            st.write(f\"Scraping {source} page {page}...\")\n",
        "            reviews = scrape_page(url)\n",
        "            if not reviews:\n",
        "                break\n",
        "            source_reviews.extend(reviews)\n",
        "            time.sleep(delay)\n",
        "\n",
        "        all_reviews.extend(source_reviews)\n",
        "        print(f\"Added {len(source_reviews)} reviews from {source}\")\n",
        "\n",
        "    return all_reviews\n",
        "\n",
        "\n",
        "# Load models function\n",
        "@st.cache_resource\n",
        "def models():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    embedding_tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\n",
        "    embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True).to(device)\n",
        "\n",
        "    model_path = \"microsoft/Phi-4-mini-instruct\"\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"cuda\",\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    st.write(\"Models loaded\")\n",
        "    return embedding_model, embedding_tokenizer, chat_model, chat_tokenizer\n",
        "\n",
        "\n",
        "# Pinecone setup and index handling\n",
        "def vector_db():\n",
        "    pine_key = \"pcsk_7TbUb2_TigxRVhcCmVZDxxrNoMZEwePrwMjEUTobVHGsKzUdzJtqMZETWwh6w4UoB4gjbk\"\n",
        "    pc = Pinecone(api_key=pine_key)\n",
        "    index_name = 'store-embeddings'\n",
        "\n",
        "    # Check if the index exists\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        # If index doesn't exist, create it\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1024,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "    time.sleep(1)\n",
        "    return index\n",
        "\n",
        "\n",
        "def check_index_status(index):\n",
        "    try:\n",
        "        index.describe()\n",
        "        return True  # Index exists\n",
        "    except Exception as e:\n",
        "        return False  # Index does not exist\n",
        "\n",
        "\n",
        "def delete_index():\n",
        "    pine_key = \"pcsk_7TbUb2_TigxRVhcCmVZDxxrNoMZEwePrwMjEUTobVHGsKzUdzJtqMZETWwh6w4UoB4gjbk\"\n",
        "    pc = Pinecone(api_key=pine_key)\n",
        "    index_name = 'store-embeddings'\n",
        "    try:\n",
        "        pc.delete_index(index_name)\n",
        "        st.write(f\"Index '{index_name}' deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error deleting index: {e}\")\n",
        "\n",
        "\n",
        "# Function to generate embeddings\n",
        "def generate_embeddings(texts, embedding_model, embedding_tokenizer):\n",
        "    embedding_model.eval()\n",
        "    with torch.no_grad():\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        inputs = embedding_tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "        outputs = embedding_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().float().numpy()\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Function to upsert reviews into Pinecone index\n",
        "def upserting(all_reviews, index, embedding_model, embedding_tokenizer):\n",
        "    batch_size = 32\n",
        "    text_splitter = SpacyTextSplitter(chunk_size=500)\n",
        "\n",
        "    for review_id, review in enumerate(all_reviews):\n",
        "        chunks = text_splitter.split_text(review)\n",
        "        chunk_embeddings = []\n",
        "        chunk_ids = []\n",
        "        chunk_metadata = []\n",
        "\n",
        "        for chunk_index, chunk in enumerate(chunks):\n",
        "            chunk_embeddings.append(chunk)\n",
        "            chunk_ids.append(f\"{review_id}_{chunk_index}\")\n",
        "            chunk_metadata.append({\"review_id\": review_id, \"chunk_index\": chunk_index, \"text\": chunk})\n",
        "\n",
        "            if len(chunk_embeddings) == batch_size:\n",
        "                embeddings = generate_embeddings(chunk_embeddings, embedding_model, embedding_tokenizer)\n",
        "                vectors = list(zip(chunk_ids, embeddings, chunk_metadata))\n",
        "                index.upsert(vectors)\n",
        "                chunk_embeddings, chunk_ids, chunk_metadata = [], [], []\n",
        "\n",
        "        if chunk_embeddings:\n",
        "            embeddings = generate_embeddings(chunk_embeddings, embedding_model, embedding_tokenizer)\n",
        "            vectors = list(zip(chunk_ids, embeddings, chunk_metadata))\n",
        "            index.upsert(vectors)\n",
        "\n",
        "    st.write(\"Upserting Completed\")\n",
        "\n",
        "\n",
        "# Retrieve context for a query\n",
        "def retrieve_context(query, index, embedding_model):\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = index.query(vector=query_embedding, top_k=10, include_metadata=True)\n",
        "    retrieved_text = \" \".join([result['metadata']['text'] for result in results['matches']])\n",
        "    return retrieved_text\n",
        "\n",
        "\n",
        "# Generate response using the Phi model\n",
        "def generate_response(query, retrieved_context, chat_model, chat_tokenizer):\n",
        "    prompt = f\"\"\"### Context:\n",
        "            {retrieved_context}\n",
        "\n",
        "    ### Instructions:\n",
        "    You are a helpful assistant answering questions based solely on the provided context.\n",
        "    Respond in the third person without referencing individuals in the context.\n",
        "    If the information needed to answer the question is not in the context, respond with \"I cannot comment on that\".\n",
        "    Do not use any knowledge beyond what is in the context.\n",
        "\n",
        "    ### Question:\n",
        "    {query}\n",
        "\n",
        "    ### Answer:\n",
        "    \"\"\"\n",
        "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = chat_model.generate(**inputs, max_new_tokens=150, temperature=0.1, do_sample=True)\n",
        "    return chat_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# UI input\n",
        "company = st.text_input(\"Enter Company Name\")\n",
        "\n",
        "# Initialize Pinecone index\n",
        "index = vector_db()\n",
        "\n",
        "# Check and display index status\n",
        "if not check_index_status(index):\n",
        "    st.write(\"Database not initiated.\")\n",
        "else:\n",
        "    st.write(f\"Current status: Index is available.\")\n",
        "\n",
        "# Session flag to check if the delete button was clicked\n",
        "if \"delete_clicked\" not in st.session_state:\n",
        "    st.session_state.delete_clicked = False\n",
        "\n",
        "# Main App Logic (only runs if delete wasn't just triggered)\n",
        "if company and not st.session_state.delete_clicked:\n",
        "    status_placeholder = st.empty()  # Placeholder for scraping status\n",
        "    reviews = []\n",
        "\n",
        "    with st.spinner(\"Scraping reviews...\"):\n",
        "        reviews = scrape_reviews(company, sources=['bestcompany', 'trustpilot'], num_pages=1)\n",
        "\n",
        "    status_placeholder.empty()\n",
        "    st.write(f\"Total reviews scraped: {len(reviews)}\")\n",
        "\n",
        "    if reviews:\n",
        "        embedding_model, embedding_tokenizer, chat_model, chat_tokenizer = models()\n",
        "        print(\"Models Loaded\")\n",
        "\n",
        "        if not check_index_status(index):\n",
        "            st.write(\"Index not found. Creating and initializing the database...\")\n",
        "            index = vector_db()\n",
        "            upserting(reviews, index, embedding_model, embedding_tokenizer)\n",
        "            st.write(\"Index created and reviews upserted.\")\n",
        "\n",
        "        query = st.text_input(\"Enter a query\")\n",
        "        if query:\n",
        "            context = retrieve_context(query, index, embedding_model)\n",
        "            response = generate_response(query, context, chat_model, chat_tokenizer)\n",
        "            st.write(\"Response\")\n",
        "            st.write(response)\n",
        "\n",
        "            # Bottom row with Delete Button on left\n",
        "            col1, _ = st.columns([1, 8])\n",
        "            with col1:\n",
        "                if st.button(\"🗑️ Delete Database\"):\n",
        "                    delete_index()\n",
        "                    st.session_state.delete_clicked = True\n",
        "                    st.rerun()\n",
        "        else:\n",
        "            st.write(\"Please enter a query\")\n",
        "else:\n",
        "    st.write(\"Please enter company name\")\n",
        "\n",
        "# Handle delete_clicked rerun behavior cleanly\n",
        "if st.session_state.delete_clicked:\n",
        "    st.write(\"Index deleted. You can now reinitialize with new data.\")\n",
        "    # Optional: Add a \"Reset\" button\n",
        "    if st.button(\"🔁 Reset App\"):\n",
        "        st.session_state.delete_clicked = False\n",
        "        st.rerun()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbkMh62T_s05",
        "outputId": "573a5a22-9f95-4a0a-fa65-71e7c5a75bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1Xh88Lmv198"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}